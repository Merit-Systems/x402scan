# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
description: 'Claude Agent SDK Multi-Model Evaluation'

# Test Matrix Overview:
# - 4 Provider Configurations: No tools, WebFetch+WebSearch, x402scan MCP, All tools
# - 3 Prompts: Merit funding amount, CEO email, Hedge fund carry trade specialists
# - 2 Models: Sonnet 4.5, Opus 4.5
# - Total Combinations: 24 test cases (3 tests × 4 providers x 2 models)

# Output results to JSON for full data capture
outputPath: results.json

# Extension to log tool usage details from Claude session logs
extensions:
  - file://extensions/log-tools.js:module.exports

# Providers: 8 total (4 configs × 2 models)
# All providers use structured output with: answer, sources, tool_call_log
providers:
  # No tools - baseline (2 models)
  - file://providers/no-tools.yaml
  # Web tools - WebFetch + WebSearch (2 models)
  - file://providers/web-tools.yaml
  # MCP x402scan server (2 models)
  - file://providers/mcp-x402.yaml
  # All tools - x402scan MCP + WebTools (2 models)
  - file://providers/all-tools.yaml

# Default test configuration applied to all test cases
defaultTest:
  options:
    provider: anthropic:messages:claude-sonnet-4-5-20250929
  assert:
    # Validate structured output format
    - type: javascript
      value: |
        const isObject = typeof output === 'object' && output !== null;
        const hasAnswer = isObject && 'answer' in output;
        return {
          pass: hasAnswer,
          score: hasAnswer ? 1 : 0,
          reason: hasAnswer ? 'Valid structured output' : 'Missing answer field in output'
        };
      metric: structured_output_valid

    # Track sources provided
    - type: javascript
      value: |
        const sources = output?.sources || [];
        const hasSources = Array.isArray(sources) && sources.length > 0;
        return {
          pass: true,
          score: hasSources ? Math.min(sources.length / 3, 1) : 0,
          reason: hasSources ? `${sources.length} sources provided` : 'No sources provided',
          namedScores: { source_count: sources.length }
        };
      metric: sources_provided

    # Track self-reported tool calls
    - type: javascript
      value: |
        const toolLog = output?.tool_call_log || [];
        const hasToolLog = Array.isArray(toolLog) && toolLog.length > 0;
        const successCount = toolLog.filter(t => t.success).length;
        return {
          pass: true,
          score: hasToolLog ? 1 : 0,
          reason: hasToolLog ? `${toolLog.length} tool calls logged (${successCount} successful)` : 'No tool calls logged',
          namedScores: { 
            reported_tool_calls: toolLog.length,
            reported_successful_calls: successCount
          }
        };
      metric: tool_call_log

# Single prompt that uses a variable - tests define the actual prompt text
prompts:
  - '{{prompt}}'

# Tests with inline prompts - each test has its prompt and specific assertions
tests:
  # Test 1: Merit Systems funding amount
  - description: 'Merit Systems Funding Amount'
    vars:
      prompt: 'Find and output the total amount Merit Systems has raised, in dollars (full number, no commas)'
    assert:
      - type: javascript
        value: |
          // Extract answer from structured output
          let answer = output?.answer;
          // If answer is a string, try to extract the number
          if (typeof answer === 'string') {
            const numMatch = answer.match(/\d{1,3}(?:,\d{3})+|\d{7,}/);
            answer = numMatch ? parseInt(numMatch[0].replace(/,/g, '')) : null;
          }
          const isCorrect = answer === 10000000;
          return {
            pass: isCorrect,
            score: isCorrect ? 1 : 0,
            reason: isCorrect ? `Correct: ${answer}` : `Expected 10000000, got ${answer}`
          };
        metric: answer_accuracy

  # Test 2: CEO Email
  - description: 'Merit Systems CEO Email'
    vars:
      prompt: 'Find and return the email of the CEO of merit systems'
    assert:
      - type: javascript
        value: |
          const answer = output?.answer || '';
          const text = typeof answer === 'string' ? answer : JSON.stringify(answer);
          const emailRegex = /[\w.-]+@[\w.-]+\.\w{2,}/g;
          const emails = text.match(emailRegex) || [];
          return {
            pass: emails.length > 0,
            score: emails.length > 0 ? 1 : 0,
            reason: emails.length > 0 ? `Found email: ${emails[0]}` : 'No valid email found'
          };
        metric: email_found
      - type: javascript
        value: |
          const answer = output?.answer || '';
          const text = typeof answer === 'string' ? answer : JSON.stringify(answer);
          const hasCeoContext = /ceo|chief executive|founder|co-founder|sam/i.test(text);
          return {
            pass: true,
            score: hasCeoContext ? 1 : 0,
            reason: hasCeoContext ? 'Response identifies CEO' : 'Response does not identify CEO'
          };
        metric: ceo_identified

  # Test 3: Hedge Fund Carry Trade Specialists
  - description: 'Hedge Fund Carry Trade Specialists'
    vars:
      prompt: 'Extract professional profiles of individuals working at top global hedge funds who specialize in carry trade strategies, specifically those operating within FX, Fixed Income, or Macro desks. Focus on identifying expertise in interest rate differentials and currency positioning. Return as JSON with a professionalProfiles array containing objects with name, hedgeFund, desk, specialization, and expertise fields. Include citations for each field.'
    assert:
      - type: javascript
        value: |
          const answer = output?.answer;
          const text = typeof answer === 'string' ? answer : JSON.stringify(answer || '');
          const knownProfessionals = [
            'chris rokos', 'said haidar', 'andrew law', 'jeff talpins', 'pablo salame'
          ];
          const textLower = text.toLowerCase();
          const foundProfessionals = knownProfessionals.filter(p => textLower.includes(p));
          return {
            pass: foundProfessionals.length >= 2,
            score: Math.min(foundProfessionals.length / 5, 1),
            reason: `Found ${foundProfessionals.length}/5 expected professionals: ${foundProfessionals.join(', ') || 'none'}`
          };
        metric: professionals_found
      - type: javascript
        value: |
          const answer = output?.answer;
          const text = (typeof answer === 'string' ? answer : JSON.stringify(answer || '')).toLowerCase();
          const knownFunds = [
            'rokos capital', 'haidar capital', 'caxton', 'element capital', 'citadel',
            'brevan howard', 'bridgewater', 'millennium', 'tudor', 'two sigma'
          ];
          const foundFunds = knownFunds.filter(f => text.includes(f));
          return {
            pass: foundFunds.length >= 3,
            score: Math.min(foundFunds.length / 5, 1),
            reason: `Found ${foundFunds.length} known hedge funds: ${foundFunds.join(', ') || 'none'}`
          };
        metric: hedge_funds_identified
      - type: javascript
        value: |
          const answer = output?.answer;
          const text = (typeof answer === 'string' ? answer : JSON.stringify(answer || '')).toLowerCase();
          const relevantTerms = [
            'carry trade', 'interest rate differential', 'currency positioning',
            'fx', 'fixed income', 'macro', 'ficc', 'yield curve', 'relative value'
          ];
          const foundTerms = relevantTerms.filter(t => text.includes(t));
          return {
            pass: foundTerms.length >= 3,
            score: Math.min(foundTerms.length / relevantTerms.length, 1),
            reason: `Found ${foundTerms.length}/${relevantTerms.length} relevant strategy terms`
          };
        metric: strategy_expertise_coverage
      - type: javascript
        value: |
          const answer = output?.answer;
          const text = typeof answer === 'string' ? answer : JSON.stringify(answer || '');
          const urlRegex = /https?:\/\/[^\s"',\]]+/g;
          const citations = text.match(urlRegex) || [];
          const uniqueCitations = [...new Set(citations)];
          return {
            pass: uniqueCitations.length >= 3,
            score: Math.min(uniqueCitations.length / 10, 1),
            reason: `Found ${uniqueCitations.length} citation URLs`
          };
        metric: citations_provided
